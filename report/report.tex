\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}

\title{Time Complexity Analysis and Parallelization Report of Stock Prediction Pipeline}
\author{Nicola Rohner}

\begin{document}

\maketitle

\section{Parallelization Report}

\subsection{Implementation}
The parallelization of the stock analysis script was implemented using Dask, a flexible library for parallel computing in Python. The key steps in the implementation were:
Setting up a Dask client, Creating a Dask bag from the list of files and Processing all files in parallel using Dask's map and compute functions.
This approach allows for file-level parallelization, where each file is processed independently and concurrently.

\subsection{Impact on Speed}
The parallelization improved the processing speed of the script, particularly when dealing with a large number of files. My test revealed:
A Speedup around 8x, performance gains were most noticeable with a higher number of available cores.

\subsection{Reflection on the Development Process}
Dask's high-level API significantly simplified the parallelization process, allowing me to maintain much of the original code structure, especially if you code in a functional style.
The parallelized version demonstrated excellent scalability, on local multi-core machines.
For very small datasets or when processing only a few files, I found that the overhead of setting up the Dask infrastructure could outweigh the benefits of parallelization. 


\section{Function-by-Function Analysis}

\begin{enumerate}
    \item \textbf{load\_data(filename)}: $O(n)$
    \begin{itemize}
        \item This function primarily involves reading the CSV file, which scales linearly with the number of rows.
    \end{itemize}
    
    \item \textbf{add\_features(df)}: $O(n)$
    \begin{itemize}
        \item The rolling window operations (MA5, MA20, LR\_Slope, RSI) and the exponential moving average (EMA) calculations for MACD all have mostly linear time complexity.
    \end{itemize}
    
    \item \textbf{feature\_selection(df)}: $O(n \cdot f)$, where $f$ is the number of features
    \begin{itemize}
        \item The SelectKBest with f\_regression algorithm has a time complexity of $O(n \cdot f)$.
    \end{itemize}
    
    \item \textbf{train\_models(df)}: $O(k \cdot n^2 \cdot m)$, where $k$ is the number of cross-validation folds, $n$ is the number of samples, and $m$ is the number of features
    This function's complexity is dominated by the cross-validation of the SVR model with RBF kernel.
    \begin{itemize}
        \item RandomForestRegressor: $O(n_{\text{trees}} \cdot n \cdot \log(n) \cdot m)$
        \item LinearRegression: $O(n \cdot m^2)$
        \item SVR (RBF kernel): $O(n^2 \cdot m)$
        \item Cross-validation multiplies each model's complexity by $k$ (number of folds)
    \end{itemize}

    \item \textbf{process\_file(filename)}: $O(k \cdot n^2 \cdot m)$, where $k$ is the number of cross-validation folds, $n$ is the number of samples, and $m$ is the number of features
    \begin{itemize}
        \item load\_data(filename): $O(n)$, where $n$ is the number of rows in the file
        \item add\_features(df): $O(n)$
        \item feature\_selection(df): $O(n \cdot m)$
        \item train\_models(df): $O(k \cdot n^2 \cdot m)$ - dominates the overall complexity
        \item File operations (os.makedirs, joblib.dump): $O(1)$ relative to data processing
        \item The overall complexity is determined by the most computationally expensive operation, train\_models
    \end{itemize}

    \item \textbf{Overall Script Complexity}: $O(F \cdot (k \cdot n^2 \cdot m) / P)$
    \begin{itemize}
        \item $F$: Total number of files to process
        \item $k$: Number of cross-validation folds
        \item $n$: Number of samples in the largest file
        \item $m$: Number of features
        \item $P$: Number of parallel processes (depends on available cores)
    \end{itemize}
    \begin{itemize}
        \item File listing operations: $O(F)$
        \item Dask bag creation: $O(F)$
        \item Parallel processing with Dask:
            \begin{itemize}
                \item Each file processed: $O(k \cdot n^2 \cdot m)$
                \item Parallel execution reduces time by factor of $P$
            \end{itemize}
        \item The dominant factor is the parallel execution of process\_file for each file
    \end{itemize}
\end{enumerate}

\subsection{Machine Learning Algorithms}
A more detailed look at the time complexities of the machine learning algorithms used:

\begin{itemize}
    \item \textbf{Linear Regression}
    \begin{itemize}
        \item Training: $O(nm^2)$
        \item Prediction: $O(m)$
        \item Generally fast for both training and prediction, especially when the number of features $(m)$ is relatively small.
    \end{itemize}
    
    \item \textbf{Support Vector Regression (SVR)}
    \begin{itemize}
        \item Training: $O(n^2m)$
        \item Prediction: $O(vm)$, where $v$ is the number of support vectors
        \item Potentially the slowest algorithm, especially for large datasets. The RBF kernel used in our script could lead to higher complexity.
    \end{itemize}
    
    \item \textbf{Random Forest Regressor}
    \begin{itemize}
        \item Training: $O(t \cdot u \cdot n \log n)$, where $u$ is the number of features considered for splitting
        \item Prediction: $O(t \log n)$
        \item Generally efficient, especially when the number of trees $(t)$ and features considered for splitting $(u)$ are not too large.
    \end{itemize}
\end{itemize}

\section{Comparison and Analysis}

\subsection{Theoretical Complexity}
Sequential version: $O(F \cdot k \cdot n^2 \cdot m)$
Parallelized version: $O(\frac{F \cdot k \cdot n^2 \cdot m}{P})$

Where:
\begin{itemize}
    \item $F$: Total number of files
    \item $k$: Number of cross-validation folds
    \item $n$: Number of samples in the largest file
    \item $m$: Number of features
    \item $P$: Number of parallel processes
\end{itemize}

\subsection{Theoretical Speedup}
The theoretical speedup of the parallelized version over the sequential version is:

\[ \text{Speedup} = \frac{O(F \cdot k \cdot n^2 \cdot m)}{O(\frac{F \cdot k \cdot n^2 \cdot m}{P})} = O(P) \]

This suggests a linear speedup with the number of processors, up to the number of files $F$.

\subsection{Analysis}
\begin{itemize}
    \item The parallelized version distributes the workload across $P$ processes, potentially reducing execution time by a factor of $P$.
    \item Ideal speedup is achieved when $P \leq F$, as each file can be processed independently.
    \item When $P > F$, additional processors may not contribute to further speedup due to the limited number of files.
    \item The space complexity increases with parallelization to $O(P \cdot n \cdot m)$, as each process requires memory for its data and models.
\end{itemize}

\subsection{Limitations}
\begin{itemize}
    \item The speedup is bounded by the number of files $F$, following Amdahl's Law.
    \item I/O operations and network latency may become bottlenecks in a distributed setting.
    \item The effectiveness of parallelization depends on the uniformity of file sizes and processing times.
\end{itemize}

\section{Sources}
\begin{enumerate}
    \item Virgolin, M. (2021). Time complexity for different machine learning algorithms. Retrieved from \url{https://marcovirgolin.github.io/extras/details_time_complexity_machine_learning_algorithms/}
    \item Dask Development Team. (n.d.). Dask Documentation. Retrieved from \url{https://docs.dask.org/en/stable/}
\end{enumerate}

\end{document}