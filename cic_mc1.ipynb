{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/borismarjanovic/price-volume-data-for-all-us-stocks-etfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 8539 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 08:49:12,674 - ERROR - Error processing file data/Stocks/cor_a-cl.us.txt: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.\n",
      "2024-10-09 08:49:40,661 - ERROR - Error processing file data/Stocks/aieq.us.txt: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.\n",
      "2024-10-09 08:50:47,144 - ERROR - Error processing file data/Stocks/gazb.us.txt: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.\n",
      "2024-10-09 08:53:50,065 - ERROR - Error processing file data/Stocks/fat.us.txt: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 155\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalysis complete. Results written to export/stock_analysis_results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Time Complexity Analysis (O-notation):\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# - Reading CSV: O(n), where n is the number of rows in the CSV file\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# - Feature engineering: O(n) for each feature\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# - O(n) for storing the model parameters\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# - Overall space complexity: O(n), where n is the number of samples in the largest file\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 142\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, filename \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(filenames):\n\u001b[0;32m--> 142\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_and_analyze_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[0;32mIn[1], line 91\u001b[0m, in \u001b[0;36mprocess_and_analyze_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Train and evaluate models\u001b[39;00m\n\u001b[1;32m     84\u001b[0m models \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     85\u001b[0m     (RandomForestRegressor, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m}),\n\u001b[1;32m     86\u001b[0m     (ElasticNet, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m}),\n\u001b[1;32m     87\u001b[0m     (SVR, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m}),\n\u001b[1;32m     88\u001b[0m     (XGBRegressor, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m})\n\u001b[1;32m     89\u001b[0m ]\n\u001b[0;32m---> 91\u001b[0m model_performances \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_selected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_selected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Find best model\u001b[39;00m\n\u001b[1;32m     97\u001b[0m best_rmse, best_cv_rmse, best_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(model_performances, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[1], line 92\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Train and evaluate models\u001b[39;00m\n\u001b[1;32m     84\u001b[0m models \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     85\u001b[0m     (RandomForestRegressor, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m}),\n\u001b[1;32m     86\u001b[0m     (ElasticNet, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m}),\n\u001b[1;32m     87\u001b[0m     (SVR, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m}),\n\u001b[1;32m     88\u001b[0m     (XGBRegressor, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m})\n\u001b[1;32m     89\u001b[0m ]\n\u001b[1;32m     91\u001b[0m model_performances \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 92\u001b[0m     \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_selected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_selected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model_class, params \u001b[38;5;129;01min\u001b[39;00m models\n\u001b[1;32m     94\u001b[0m ]\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Find best model\u001b[39;00m\n\u001b[1;32m     97\u001b[0m best_rmse, best_cv_rmse, best_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(model_performances, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(X_train, X_test, y_train, y_test, model_class, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_evaluate_model\u001b[39m(X_train, X_test, y_train, y_test, model_class, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     23\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_class(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     26\u001b[0m     mse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_test, y_pred)\n",
      "File \u001b[0;32m~/Documents/GitHub/cic_mc1/.venv/lib/python3.11/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/cic_mc1/.venv/lib/python3.11/site-packages/xgboost/sklearn.py:1108\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m-> 1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/cic_mc1/.venv/lib/python3.11/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/cic_mc1/.venv/lib/python3.11/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/cic_mc1/.venv/lib/python3.11/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def extract_symbol(filename):\n",
    "    return os.path.splitext(os.path.basename(filename))[0].split('.')[0]\n",
    "\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, model_class, **kwargs):\n",
    "    model = model_class(**kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "    \n",
    "    return rmse, cv_rmse, model.__class__.__name__\n",
    "\n",
    "def process_and_analyze_file(filename):\n",
    "    try:\n",
    "        if os.path.getsize(filename) == 0:\n",
    "            logging.warning(f\"Skipping empty file: {filename}\")\n",
    "            return None\n",
    "        \n",
    "        symbol = extract_symbol(filename)\n",
    "        df = pd.read_csv(filename, parse_dates=['Date'], usecols=['Date', 'Close', 'High', 'Low', 'Volume'])\n",
    "        \n",
    "        if df.empty:\n",
    "            logging.warning(f\"File {filename} is empty after reading.\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate daily returns\n",
    "        df['Return'] = df['Close'].pct_change()\n",
    "        \n",
    "        # Feature engineering\n",
    "        df['MA5'] = df['Close'].rolling(window=5).mean()\n",
    "        df['MA20'] = df['Close'].rolling(window=20).mean()\n",
    "        df['Volatility'] = df['Return'].rolling(window=20).std()\n",
    "        \n",
    "        # Additional features\n",
    "        df['Slope'] = (df['Close'] - df['Close'].shift(5)) / 5\n",
    "        df['RSI'] = calculate_rsi(df['Close'], window=14)\n",
    "        df['MACD'] = calculate_macd(df['Close'])\n",
    "        \n",
    "        # Prepare data for ML models\n",
    "        df = df.dropna()\n",
    "        features = ['Close', 'High', 'Low', 'Volume', 'Return', 'MA5', 'MA20', 'Volatility', 'Slope', 'RSI', 'MACD']\n",
    "        X = df[features]\n",
    "        y = df['Close'].shift(-1).dropna()\n",
    "        X = X[:-1]  # Remove last row to align with y\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Feature selection\n",
    "        selector = SelectKBest(score_func=f_regression, k=5)\n",
    "        X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "        X_test_selected = selector.transform(X_test_scaled)\n",
    "        \n",
    "        selected_features = [features[i] for i in selector.get_support(indices=True)]\n",
    "        \n",
    "        # Train and evaluate models\n",
    "        models = [\n",
    "            (RandomForestRegressor, {'n_estimators': 100, 'random_state': 42}),\n",
    "            (ElasticNet, {'alpha': 1.0, 'random_state': 42}),\n",
    "            (SVR, {'kernel': 'rbf'}),\n",
    "            (XGBRegressor, {'n_estimators': 100, 'random_state': 42})\n",
    "        ]\n",
    "        \n",
    "        model_performances = [\n",
    "            train_and_evaluate_model(X_train_selected, X_test_selected, y_train, y_test, model_class, **params) \n",
    "            for model_class, params in models\n",
    "        ]\n",
    "        \n",
    "        # Find best model\n",
    "        best_rmse, best_cv_rmse, best_model = min(model_performances, key=lambda x: x[0])\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        summary = {\n",
    "            'symbol': symbol,\n",
    "            'avg_daily_return': df['Return'].mean(),\n",
    "            'volatility': df['Return'].std() * np.sqrt(252),  # Annualized volatility\n",
    "            'sharpe_ratio': df['Return'].mean() / df['Return'].std() * np.sqrt(252),\n",
    "            'max_drawdown': (df['Close'] / df['Close'].cummax() - 1).min(),\n",
    "            'avg_volume': df['Volume'].mean(),\n",
    "            'best_model': best_model,\n",
    "            'best_model_rmse': best_rmse,\n",
    "            'best_model_cv_rmse': best_cv_rmse,\n",
    "            'selected_features': ', '.join(selected_features)\n",
    "        }\n",
    "        \n",
    "        return pd.Series(summary)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file {filename}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def calculate_rsi(prices, window=14):\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_macd(prices, slow=26, fast=12, signal=9):\n",
    "    ema_slow = prices.ewm(span=slow, adjust=False).mean()\n",
    "    ema_fast = prices.ewm(span=fast, adjust=False).mean()\n",
    "    macd = ema_fast - ema_slow\n",
    "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
    "    return macd - signal_line\n",
    "\n",
    "def main():\n",
    "    # Get list of filenames data/Stocks/*.txt and data/ETFs/*.txt\n",
    "    file_pattern = 'data/**/*.txt'\n",
    "    filenames = glob.glob(file_pattern)\n",
    "    print(f\"Processing {len(filenames)} files\")\n",
    "\n",
    "    # Process files serially\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    for i, filename in enumerate(filenames):\n",
    "        result = process_and_analyze_file(filename)\n",
    "        if result is not None:\n",
    "            results.append(result)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    # Write results to CSV\n",
    "    df_results.to_csv('export/stock_analysis_results.csv', index=False)\n",
    "\n",
    "    print(\"Analysis complete. Results written to export/stock_analysis_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Time Complexity Analysis (O-notation):\n",
    " - Reading CSV: O(n), where n is the number of rows in the CSV file\n",
    " - Feature engineering: O(n) for each feature\n",
    " - Train-test split: O(n)\n",
    " - StandardScaler: O(n)\n",
    " - Feature selection: O(n * k), where k is the number of features\n",
    " - Model training: \n",
    "   - RandomForest: O(n_trees * n_features * n_samples * log(n_samples))\n",
    "   - ElasticNet: O(n_features * n_samples)\n",
    "   - SVR: O(n_samples^2 * n_features)\n",
    "   - XGBoost: O(n_trees * n_features * n_samples * log(n_samples))\n",
    " - Cross-validation: O(k * n), where k is the number of folds\n",
    " - Overall time complexity: O(m * (n * log(n))), where m is the number of files and n is the number of samples in each file\n",
    " Space Complexity:\n",
    " - O(n) for storing the DataFrame and feature arrays\n",
    " - O(n) for storing the model parameters\n",
    " - Overall space complexity: O(n), where n is the number of samples in the largest file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Took 178 Minutes without paralelization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
