{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/borismarjanovic/price-volume-data-for-all-us-stocks-etfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:06<01:00,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/stock\\a.us.txt, best model: LinearRegression, features: ['MA5', 'MA20', 'MACD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:31<02:16, 17.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/stock\\aa.us.txt, best model: LinearRegression, features: ['MA5', 'MA20', 'MACD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:31<01:07,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/stock\\aaap.us.txt, best model: LinearRegression, features: ['MA5', 'MA20', 'MACD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:40<00:55,  9.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/stock\\aaba.us.txt, best model: LinearRegression, features: ['MA5', 'MA20', 'MACD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:41<00:31,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/stock\\aac.us.txt, best model: LinearRegression, features: ['MA5', 'MA20', 'MACD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:42<00:18,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/stock\\aal.us.txt, best model: LinearRegression, features: ['MA5', 'MA20', 'MACD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:44<00:10,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/stock\\aamc.us.txt, best model: LinearRegression, features: ['MA5', 'MA20', 'MACD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:48<00:07,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/stock\\aame.us.txt, best model: LinearRegression, features: ['MA5', 'MA20', 'MACD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:54<00:04,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/stock\\aan.us.txt, best model: LinearRegression, features: ['MA5', 'MA20', 'MACD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:56<00:00,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/stock\\aaoi.us.txt, best model: LinearRegression, features: ['MA5', 'MA20', 'MACD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "def load_data(filename):\n",
    "    try:\n",
    "        df = pd.read_csv(filename, parse_dates=['Date'])\n",
    "        if df.empty:\n",
    "            raise ValueError(\"File is empty\")\n",
    "        df.set_index('Date', inplace=True)\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError:\n",
    "        raise ValueError(\"File is empty\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading file: {str(e)}\")\n",
    "\n",
    "def add_features(df):\n",
    "    df['MA5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['MA20'] = df['Close'].rolling(window=20).mean()\n",
    "    df['LR_Slope'] = df['Close'].rolling(window=20).apply(lambda x: np.polyfit(range(20), x, 1)[0])\n",
    "    \n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    exp1 = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = exp1 - exp2\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "def feature_selection(df):\n",
    "    X = df[['MA5', 'MA20', 'LR_Slope', 'RSI', 'MACD']]\n",
    "    y = df['Close']\n",
    "    selector = SelectKBest(score_func=f_regression, k=3)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    return df[selected_features + ['Close']]\n",
    "\n",
    "def train_models(df):\n",
    "    X = df.drop('Close', axis=1)\n",
    "    y = df['Close']\n",
    "    \n",
    "    models = {\n",
    "        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'LinearRegression': LinearRegression(),\n",
    "        'SVR': SVR(kernel='rbf')\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "        results[name] = -scores.mean()\n",
    "    \n",
    "    best_model_name = min(results, key=results.get)\n",
    "    best_model = models[best_model_name]\n",
    "    best_model.fit(X, y)\n",
    "    \n",
    "    return best_model, best_model_name, X.columns.tolist()\n",
    "\n",
    "def process_file(filename):\n",
    "    try:\n",
    "        df = load_data(filename)\n",
    "        if len(df) < 20:  # Not enough data to compute features\n",
    "            return f\"Skipped {filename}: Not enough data\"\n",
    "        \n",
    "        df = add_features(df)\n",
    "        df = feature_selection(df)\n",
    "        best_model, model_name, features = train_models(df)\n",
    "        \n",
    "        output_dir = 'output'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        base_filename = os.path.splitext(os.path.basename(filename))[0]\n",
    "        model_filename = f\"{output_dir}/{base_filename}_model.joblib\"\n",
    "        joblib.dump((best_model, model_name, features), model_filename)\n",
    "        \n",
    "        return f\"Processed {filename}, best model: {model_name}, features: {features}\"\n",
    "    except ValueError as e:\n",
    "        return f\"Skipped {filename}: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing {filename}: {str(e)}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get all file paths\n",
    "    stock_files = [os.path.join('data/stock', f) for f in os.listdir('data/stock') if f.endswith('.txt')]\n",
    "    etf_files = [os.path.join('data/etfs', f) for f in os.listdir('data/etfs') if f.endswith('.txt')]\n",
    "    all_files = stock_files + etf_files\n",
    "    \n",
    "    # Process all files sequentially\n",
    "    for file in tqdm(all_files[:10]):\n",
    "        result = process_file(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say it would take around 1 Minute to run 10 Files. So, for 8539 Files, it would take around 853.9 Minutes.\n",
    "We can derive the time it would take to run 8539 Files using the following formula:\n",
    "\n",
    "Complexity of the code is $O(F \\cdot k \\cdot n^2 \\cdot m)$, where $F$ is the number of files, $k$ is the number of stocks, $n$ is the number of days, and $m$ is the number of features.\n",
    "\n",
    "Complexity of the parallel code is $O\\left(\\frac{F \\cdot k \\cdot n^2 \\cdot m}{P}\\right)$, where $P$ is the number of processes.\n",
    "Which in my case is 16. So, the speedup would be:\n",
    "\n",
    "$$\n",
    "\\text{Speedup} = \\frac{O(F \\cdot k \\cdot n^2 \\cdot m)}{O\\left(\\frac{F \\cdot k \\cdot n^2 \\cdot m}{P}\\right)}\n",
    "$$\n",
    "\n",
    "We know that the parallel code took around 57 minutes to run 8539 files using 16 processors. So we can calculate the time it would take for the non-parallel code to run 8539 files:\n",
    "\n",
    "$$\n",
    "\\text{Time for serial code} = 16 \\times \\text{Time for parallel code}\n",
    "$$\n",
    "\n",
    "Substituting the values:\n",
    "\n",
    "$$\n",
    "\\text{Time for serial code} = 16 \\times 57 \\text{ minutes} = 912 \\text{ minutes}\n",
    "$$\n",
    "\n",
    "Therefore, the non-parallel code would take approximately 912 minutes to process 8539 files.\n",
    "Which is pretty close to our assumption of 853.9 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\socia\\Documents\\GitHub\\cic_mc1\\.venv\\Lib\\site-packages\\distributed\\node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 51642 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask dashboard available at: http://127.0.0.1:51642/status\n",
      "Total files: 8539\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "def load_data(filename: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_csv(filename, parse_dates=['Date'])\n",
    "        if df.empty:\n",
    "            raise ValueError(\"File is empty\")\n",
    "        df.set_index('Date', inplace=True)\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError:\n",
    "        raise ValueError(\"File is empty\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading file: {str(e)}\")\n",
    "\n",
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['MA5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['MA20'] = df['Close'].rolling(window=20).mean()\n",
    "    df['LR_Slope'] = df['Close'].rolling(window=20).apply(lambda x: np.polyfit(range(20), x, 1)[0])\n",
    "    \n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    exp1 = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = exp1 - exp2\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "def feature_selection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = df[['MA5', 'MA20', 'LR_Slope', 'RSI', 'MACD']]\n",
    "    y = df['Close']\n",
    "    selector = SelectKBest(score_func=f_regression, k=3)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    return df[selected_features + ['Close']]\n",
    "\n",
    "def train_models(df: pd.DataFrame) -> tuple:\n",
    "    X = df.drop('Close', axis=1)\n",
    "    y = df['Close']\n",
    "    \n",
    "    models = {\n",
    "        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'LinearRegression': LinearRegression(),\n",
    "        'SVR': SVR(kernel='rbf')\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "        results[name] = -scores.mean()\n",
    "    \n",
    "    best_model_name = min(results, key=results.get)\n",
    "    best_model = models[best_model_name]\n",
    "    best_model.fit(X, y)\n",
    "    \n",
    "    return best_model, best_model_name, X.columns.tolist()\n",
    "\n",
    "def process_file(filename: str) -> str:\n",
    "    try:\n",
    "        df = load_data(filename)\n",
    "        if len(df) < 20:  # Not enough data to compute features\n",
    "            return f\"Skipped {filename}: Not enough data\"\n",
    "        \n",
    "        df = add_features(df)\n",
    "        df = feature_selection(df)\n",
    "        best_model, model_name, features = train_models(df)\n",
    "        \n",
    "        output_dir = 'output'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        base_filename = os.path.splitext(os.path.basename(filename))[0]\n",
    "        model_filename = f\"{output_dir}/{base_filename}_model.joblib\"\n",
    "        joblib.dump((best_model, model_name, features), model_filename)\n",
    "        \n",
    "        return f\"Processed {filename}, best model: {model_name}, features: {features}\"\n",
    "    except ValueError as e:\n",
    "        return f\"Skipped {filename}: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing {filename}: {str(e)}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up Dask client\n",
    "    cluster = LocalCluster()\n",
    "    client = Client(cluster)\n",
    "    print(f\"Dask dashboard available at: {client.dashboard_link}\")\n",
    "    \n",
    "    # Get all file paths\n",
    "    stock_files = [os.path.join('data/stock', f) for f in os.listdir('data/stock') if f.endswith('.txt')]\n",
    "    etf_files = [os.path.join('data/etfs', f) for f in os.listdir('data/etfs') if f.endswith('.txt')]\n",
    "    all_files = stock_files + etf_files\n",
    "    print(f\"Total files: {len(all_files)}\")\n",
    "    # Create a Dask bag from the file list\n",
    "    bag = db.from_sequence(all_files)\n",
    "    \n",
    "    # Process all files in parallel\n",
    "    bag.map(process_file).compute()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
